{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201e291f-2248-487d-b99c-a957fc871360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets, backend\n",
    "from tensorflow.keras.layers import Conv2D, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0763c112-3986-49ea-9158-44603d640bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el directorio con los archivos .h5\n",
    "directory = \"./BraTS2020_training_data/content/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd034cfb-40a7-42b0-8a52-bbab0e7a488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivos a array\n",
    "h5_files = [f for f in os.listdir(directory) if f.endswith('.h5')]\n",
    "print(f\"Found {len(h5_files)} .h5 files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c955f-0269-4491-98c2-3ab2cf39e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir los primeros archivos .h5 de la lista para inspeccionarlos\n",
    "if h5_files:\n",
    "    selected_file = random.choice(h5_files)\n",
    "    file_path = os.path.join(directory, selected_file)\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        print(\"\\nKeys for each file:\", list(file.keys()))\n",
    "        for key in file.keys():\n",
    "            print(f\"\\nData type of {key}:\", type(file[key][()]))\n",
    "            print(f\"Shape of {key}:\", file[key].shape)\n",
    "            print(f\"Array dtype: {file[key].dtype}\")\n",
    "            print(f\"Array max val: {np.max(file[key])}\")\n",
    "            print(f\"Array min val: {np.min(file[key])}\")\n",
    "            print(\"*\"*10)\n",
    "            print(f\"Mean: {np.mean(file[key])}\")\n",
    "            print(f\"Standard deviation: {np.std(file[key])}\")\n",
    "            print(\"*\"*10)\n",
    "            \n",
    "            # Verificar valores nulos\n",
    "            if np.isnan(file[key]).any():\n",
    "                print(\"Hay valores NaN en los datos.\")\n",
    "            else:\n",
    "                print(\"No se encontraron valores NaN.\")\n",
    "else:\n",
    "    print(\"No .h5 files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c64e3d-4327-476b-9f6d-a86b62b53d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar datos\n",
    "def visualize_image_and_masks(image, mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: Tensor o array de la imagen (H, W, C).\n",
    "        mask: Tensor o array de la máscara (H, W) o (H, W, C).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Mostrar imagen\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Imagen\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Mostrar máscara (asumiendo que está codificada en un solo canal)\n",
    "    if len(mask.shape) == 3 and mask.shape[-1] > 1:\n",
    "        mask_display = tf.argmax(mask, axis=-1)  # Reducir la máscara a un solo canal si está en one-hot encoding\n",
    "    else:\n",
    "        mask_display = mask\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask_display, cmap=\"jet\")\n",
    "    plt.title(\"Máscara\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663766b6-74ab-438e-b6d9-a1b91820c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivos\n",
    "h5_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(h5_files)\n",
    "\n",
    "# Dividir dataset en entrenaminto y validación (80:10)\n",
    "split_idx = int(0.8 * len(h5_files))\n",
    "train_files = h5_files[:split_idx]\n",
    "val_files = h5_files[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29fe86e-570e-45b3-a387-3b7dba5c39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "def preprocess(h5_file):\n",
    "    with h5py.File(h5_file.numpy().decode('utf-8'), 'r') as file:\n",
    "        image = file['image'][()]\n",
    "        mask = file['mask'][()]\n",
    "        \n",
    "        # Reescalar la imagen: (H, W, C) -> (C, H, W)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        mask = mask.transpose((2, 0, 1))\n",
    "\n",
    "        # Ajustar los valores de los píxeles en la imagen para que estén entre 0 y 1\n",
    "        for i in range(image.shape[0]):\n",
    "            min_val = np.min(image[i])\n",
    "            image[i] = image[i] - min_val\n",
    "            max_val = np.max(image[i]) + 1e-4\n",
    "            image[i] = image[i] / max_val\n",
    "\n",
    "        # Reescalar la imagen: (C, H, W) -> (H, W, C) para ser ompatibles con TensorShape\n",
    "        image = image.transpose((1, 2, 0))\n",
    "        mask = mask.transpose((1, 2, 0))\n",
    "\n",
    "        # Normalizar entre 0 y 1\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        mask = tf.cast(mask, tf.float32) / 255.0\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388b7757-221c-4c8c-8367-4b3a3a29672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer las formas después de tf.py_function\n",
    "def preprocess_with_shape(h5_file):\n",
    "    image, mask = tf.py_function(preprocess, [h5_file], [tf.float32, tf.float32])\n",
    "    image.set_shape((240, 240, 4))  # Forma de la imagen: (C, H, W)\n",
    "    mask.set_shape((240, 240, 3))   # Forma de la máscara: (C, H, W)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29027404-0708-4c7d-8c52-e379f6de0208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets para entrenamiento y validación\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "train_dataset = train_dataset.map(preprocess_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_files)\n",
    "val_dataset = val_dataset.map(preprocess_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(16).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288cfbd-4b7f-40a7-9e34-f67214015141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver ejemplos del dataset\n",
    "for image, mask in train_dataset.take(1):\n",
    "    print(\"Forma de la imagen de entrenamiento:\", image.shape)\n",
    "    print(\"Forma de la máscara de entrenamiento:\", mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ffda692-3828-4e27-9a34-cd8b0586dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque SE-ResNet\n",
    "def se_resnet_block(input_tensor, n_filters, kernel_size=3, stride=1, reduction_ratio=16):\n",
    "    # Bloque Residual\n",
    "    # Primera convolución\n",
    "    x = layers.Conv2D(n_filters, kernel_size, strides=stride, padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Segunda convolución\n",
    "    x = layers.Conv2D(n_filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Bloque SE\n",
    "    # Squeeze - Global Average Pooling\n",
    "    se = layers.GlobalAveragePooling2D()(x)\n",
    "    se = layers.Dense(n_filters // reduction_ratio, activation='relu')(se) # Bottleneck\n",
    "    # se = layers.Dropout(0.2)(se)\n",
    "    se = layers.Dense(n_filters, activation='sigmoid')(se) # Excitación\n",
    "    se = layers.Reshape((1, 1 , n_filters))(se) # Ajuste de dimensiones\n",
    "    x = layers.Multiply()([x, se]) # Recalibración\n",
    "\n",
    "    # Shortcut connection - Identity\n",
    "    if input_tensor.shape[-1] != n_filters:\n",
    "        shortcut = layers.Conv2D(n_filters, (1, 1), padding='same')(input_tensor)\n",
    "    else:\n",
    "        shortcut = input_tensor\n",
    "\n",
    "    x = layers.Add()([x, shortcut])  # Suma residual\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22dace63-5a9f-4fcd-b808-050180d6cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Gate\n",
    "def attention_gate(dec, enc, n_filters):\n",
    "    # Reducir dimensión de Encoder\n",
    "    enc1 = layers.Conv2D(n_filters, (1, 1), padding='same')(enc)\n",
    "    enc1 = layers.BatchNormalization()(enc1)\n",
    "\n",
    "    # Reducir dimensión de Gatting Signal\n",
    "    gatting_sig = layers.Conv2D(n_filters, (1, 1), padding='same')(dec)\n",
    "    gatting_sig = layers.BatchNormalization()(gatting_sig)\n",
    "\n",
    "    # Combinar Gatting Signal y Skip Connection\n",
    "    combined = layers.Add()([enc1, gatting_sig])\n",
    "    combined = layers.ReLU()(combined)\n",
    "\n",
    "    # Mapa de Atención\n",
    "    attention_map = layers.Conv2D(1, (1, 1), activation='sigmoid',padding='same')(combined)\n",
    "\n",
    "    # Aplicar mapa de Atención\n",
    "    output = layers.Multiply()([enc, attention_map])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bde7e78-4431-458c-9887-b8a46f4affc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "def encoder_block(input_tensor, n_filters, apply_pooling=True):\n",
    "    x = se_resnet_block(input_tensor, n_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    return x, p # x se utiliza en el skip connection; p pasa al siguiente bloque\n",
    "    \n",
    "# Downsampling\n",
    "def build_encoder(input_tensor, filters_list):\n",
    "    skips = []\n",
    "    \n",
    "    # Primera Convolución\n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same')(input_tensor)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    for i, n_filters in enumerate(filters_list):\n",
    "        # Verifica si es el último bloque\n",
    "        apply_pooling = i < len(filters_list) - 1  # True excepto en el último bloque\n",
    "        x, p = encoder_block(x, n_filters, apply_pooling=apply_pooling)\n",
    "        skips.append(x)  # Guardar skip connection\n",
    "        \n",
    "        # Avanzar al siguiente bloque solo si hay pooling\n",
    "        if apply_pooling:\n",
    "            x = p\n",
    "\n",
    "    return x, skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caee365e-a7e5-4dc0-9dc7-9f258bc66c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "def decoder_block(input_tensor, skip_tensor, n_filters):\n",
    "    # Gatting Signal\n",
    "    gated_skip = attention_gate(input_tensor, skip_tensor, n_filters)\n",
    "    gated_skip = layers.Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(gated_skip)\n",
    "\n",
    "    x = layers.Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor) # Upsampling\n",
    "    x = layers.Concatenate()([x, gated_skip])  # Combina con skip connection\n",
    "    x = se_resnet_block(x, n_filters)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Upsampling\n",
    "def build_decoder(encoder_output, skips, filters_list):\n",
    "    x = encoder_output\n",
    "    for filters, skip_tensor in zip(filters_list, reversed(skips)):        \n",
    "        x = decoder_block(x, skip_tensor, filters)\n",
    "\n",
    "    x = layers.Conv2D(3, (1, 1), strides=1, padding='same', activation='softmax')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f860d77-cf15-431b-ae64-a863636c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE-ResANet\n",
    "def se_resanet(input_shape, encoder_filters, decoder_filters):\n",
    "    input_tensor = layers.Input(shape=input_shape)\n",
    "    # input_tensor = layers.InputLayer(input_shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_output, skips = build_encoder(input_tensor, encoder_filters)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_output = build_decoder(encoder_output, skips, decoder_filters)\n",
    "\n",
    "    # Output\n",
    "    # outputs = layers.Conv2D(filters=4, kernel_size=1, strides=1, padding='same', activation='softmax')(decoder_output)\n",
    "    outputs = layers.Conv2D(3, (1, 1), activation='softmax')(decoder_output)\n",
    "\n",
    "    # Modelo\n",
    "    return models.Model(inputs=input_tensor, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb007fec-05ac-4191-9cef-a19d9d3732f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creación del modelo\n",
    "input_shape = (240, 240, 4)  # Dimensiones de las imágenes BraTS2020\n",
    "\n",
    "encoder_filters = [64, 128, 256, 512] # Filtros de cada nivel del Encoder\n",
    "decoder_filters = [512, 256, 128, 64] # Filtros de cada nivel del Decoder\n",
    "\n",
    "model = se_resanet(input_shape, encoder_filters, decoder_filters)\n",
    "\n",
    "# Resumen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcc38273-e097-4512-be1a-32b29e37252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Coefficient\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = backend.flatten(y_true)\n",
    "    y_pred_f = backend.flatten(y_pred)\n",
    "    intersection = backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (backend.sum(y_true_f) + backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Recall\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = backend.sum(backend.round(backend.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87036414-8eeb-45f8-979f-3aa9cc3bad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # Optimizar con Adam\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', dice_coefficient, recall]  # Métrica de precisión\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e067e4f1-61f4-4e96-bbca-52fc23548154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "class CustomMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Imprimir las métricas con 4 decimales\n",
    "        metrics = {key: f\"{value:.4f}\" for key, value in logs.items()}\n",
    "        print(f\"Epoch {epoch + 1}: {metrics}\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        # Mostrar las métricas finales al terminar el entrenamiento\n",
    "        print(\"\\nEntrenamiento terminado. Métricas finales:\")\n",
    "        for key, value in self.model.history.history.items():\n",
    "            total = sum(value) / len(value)\n",
    "            print(f\"{key}: {total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8da401-fd27-4723-986e-ca5e3a5c8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, CustomMetricsCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76806af-403f-48e6-bf71-0ead083da1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "val_loss, val_accuracy, val_dice, val_recall = model.evaluate(val_dataset)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Dice Coefficient: {val_dice}\")\n",
    "print(f\"Validation Recall: {val_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e094105-9869-4834-9de4-47488b2b5bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
